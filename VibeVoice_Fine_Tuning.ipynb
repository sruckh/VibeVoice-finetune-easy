{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéôÔ∏è VibeVoice Fine-tuning on Google Colab\n",
        "\n",
        "Easy fine-tuning for VibeVoice speech synthesis models using LoRA.\n",
        "\n",
        "**Features:**\n",
        "- ‚ö° One-click setup\n",
        "- üìÅ Easy dataset upload\n",
        "- üéØ Preset training configurations\n",
        "- üíæ Automatic model saving to Google Drive\n",
        "\n",
        "**Hardware Requirements:**\n",
        "- VibeVoice 1.5B: Works on T4 (16GB)\n",
        "- VibeVoice 7B: Requires A100 (40GB+)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Step 0: Runtime Setup\n",
        "\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **GPU** as the hardware accelerator\n",
        "3. Click **Save**\n",
        "4. Run the cell below to verify GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 1: Install Dependencies\n",
        "\n",
        "This will clone the repository and install all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Install VibeVoice Fine-tuning Environment\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = Path(\"/content/vibevoice-finetune\")\n",
        "REPO_DIR = BASE_DIR / \"VibeVoice-finetuning\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [BASE_DIR, MODELS_DIR, DATA_DIR, OUTPUT_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Created directories\")\n",
        "\n",
        "# Clone repository if not exists\n",
        "if not REPO_DIR.exists():\n",
        "    print(\"üì• Cloning VibeVoice-finetuning repository...\")\n",
        "    !git clone https://github.com/voicepowered-ai/VibeVoice-finetuning.git {REPO_DIR}\n",
        "else:\n",
        "    print(\"‚úÖ Repository already exists\")\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "\n",
        "# Install PyTorch with CUDA\n",
        "!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu126

        # Install Flash Attention (pre-built wheel for CUDA 12, PyTorch 2.9)
        !pip install -q https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\n",
        "\n",
        "# Install VibeVoice package\n",
        "%cd {REPO_DIR}\n",
        "!pip install -q -e .\n",
        "\n",
        "# Install compatible transformers\n",
        "!pip uninstall -y transformers -q\n",
        "!pip install -q transformers==4.51.3\n",
        "\n",
        "# Install additional utilities\n",
        "!pip install -q \\\n",
        "    datasets \\\n",
        "    soundfile \\\n",
        "    librosa \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    wandb \\\n",
        "    tensorboard \\\n",
        "    pydub \\\n",
        "    audioread \\\n",
        "    openai-whisper \\\n",
        "    huggingface-hub\n",
        "\n",
        "%cd /content\n",
        "print(\"\\n‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Step 2: Download Model\n",
        "\n",
        "Choose which VibeVoice model to fine-tune:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Select and Download Model\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Model options\n",
        "models = {\n",
        "    \"VibeVoice-Base (1.5B) - 16GB VRAM\": \"aoi-ot/VibeVoice-Base\",\n",
        "    \"VibeVoice-Large (7B) - 48GB VRAM\": \"aoi-ot/VibeVoice-Large\"\n",
        "}\n",
        "\n",
        "# Create dropdown\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=list(models.keys()),\n",
        "    value=list(models.keys())[0],\n",
        "    description='Model:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "display(model_dropdown)\n",
        "\n",
        "def download_model(b):\n",
        "    model_name = models[model_dropdown.value]\n",
        "    model_dir = MODELS_DIR / model_name.replace(\"/\", \"--\")\n",
        "    \n",
        "    print(f\"üì• Downloading {model_dropdown.value}...\")\n",
        "    print(f\"   This may take a few minutes...\")\n",
        "    \n",
        "    snapshot_download(\n",
        "        repo_id=model_name,\n",
        "        local_dir=model_dir,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Model downloaded to: {model_dir}\")\n",
        "    return model_dir\n",
        "\n",
        "download_btn = widgets.Button(\n",
        "    description='Download Model',\n",
        "    button_style='primary'\n",
        ")\n",
        "download_btn.on_click(download_model)\n",
        "display(download_btn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 3: Upload Your Dataset\n",
        "\n",
        "### Option A: Upload Audio Files Directly\n",
        "Upload your audio files (.wav, .mp3, .flac) and optionally transcript files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Upload Audio Files\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "audio_upload_dir = DATA_DIR / \"audio\"\n",
        "audio_upload_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"üì§ Upload your audio files (.wav, .mp3, .flac)...\")\n",
        "print(\"(You can select multiple files)\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    filepath = audio_upload_dir / filename\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(content)\n",
        "    print(f\"‚úÖ Saved: {filename}\")\n",
        "\n",
        "print(f\"\\nüìÅ Files saved to: {audio_upload_dir}\")\n",
        "print(f\"üìä Total files: {len(list(audio_upload_dir.glob('*')))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Mount Google Drive\n",
        "If your dataset is already in Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"üîå Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted at /content/drive\")\n",
        "\n",
        "# Show common paths\n",
        "print(\"\\nüìÇ Common paths:\")\n",
        "print(\"  My Drive: /content/drive/MyDrive\")\n",
        "print(\"  Datasets: /content/drive/MyDrive/datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option C: Use HuggingFace Dataset\n",
        "Use a dataset directly from HuggingFace Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Use HuggingFace Dataset\n",
        "\n",
        "hf_dataset_name = \"\"  # @param {type: \"string\", placeholder: \"username/dataset-name\"}\n",
        "\n",
        "if hf_dataset_name:\n",
        "    print(f\"üì• Loading dataset: {hf_dataset_name}\")\n",
        "    from datasets import load_dataset\n",
        "    dataset = load_dataset(hf_dataset_name)\n",
        "    print(f\"‚úÖ Dataset loaded!\")\n",
        "    print(f\"   Splits: {list(dataset.keys())}\")\n",
        "    print(f\"   Train size: {len(dataset['train'])}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No dataset specified. Skipping...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 4: Prepare Dataset\n",
        "\n",
        "Convert your audio files to the required JSONL format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prepare Dataset Configuration\n",
        "\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# @markdown ### Dataset Source\n",
        "dataset_source = \"uploaded_audio\"  # @param [\"uploaded_audio\", \"google_drive\", \"huggingface\"]\n",
        "\n",
        "# @markdown ### Paths (modify if using Google Drive)\n",
        "audio_dir = \"/content/vibevoice-finetune/data/audio\"  # @param {type: \"string\"}\n",
        "transcript_dir = \"\"  # @param {type: \"string\", placeholder: \"Leave empty to auto-transcribe\"}\n",
        "\n",
        "# @markdown ### Options\n",
        "auto_transcribe = True  # @param {type: \"boolean\"}\n",
        "whisper_model = \"base\"  # @param [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "speaker_prefix = \"Speaker 0\"  # @param {type: \"string\"}\n",
        "val_split = 0.1  # @param {type: \"slider\", min: 0.0, max: 0.3, step: 0.05}\n",
        "\n",
        "output_jsonl = DATA_DIR / \"dataset.jsonl\"\n",
        "\n",
        "print(\"‚öôÔ∏è Dataset Configuration:\")\n",
        "print(f\"  Source: {dataset_source}\")\n",
        "print(f\"  Audio directory: {audio_dir}\")\n",
        "print(f\"  Auto-transcribe: {auto_transcribe}\")\n",
        "if auto_transcribe:\n",
        "    print(f\"  Whisper model: {whisper_model}\")\n",
        "print(f\"  Speaker prefix: {speaker_prefix}\")\n",
        "print(f\"  Validation split: {val_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run Dataset Preparation\n",
        "\n",
        "import whisper\n",
        "import soundfile as sf\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def validate_audio(filepath):\n",
        "    \"\"\"Validate audio file and return metadata\"\"\"\n",
        "    try:\n",
        "        info = sf.info(filepath)\n",
        "        return {\n",
        "            'valid': True,\n",
        "            'duration': info.duration,\n",
        "            'sample_rate': info.samplerate,\n",
        "            'channels': info.channels\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'valid': False, 'error': str(e)}\n",
        "\n",
        "def transcribe_audio(audio_path, model):\n",
        "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
        "    try:\n",
        "        result = model.transcribe(str(audio_path))\n",
        "        return result[\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Transcription error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Find audio files\n",
        "audio_path = Path(audio_dir)\n",
        "audio_extensions = ('.wav', '.mp3', '.flac', '.ogg', '.m4a')\n",
        "audio_files = []\n",
        "for ext in audio_extensions:\n",
        "    audio_files.extend(audio_path.rglob(f'*{ext}'))\n",
        "\n",
        "audio_files = sorted(audio_files)\n",
        "print(f\"üìÅ Found {len(audio_files)} audio files\\n\")\n",
        "\n",
        "if not audio_files:\n",
        "    print(\"‚ùå No audio files found! Please upload audio files first.\")\n",
        "else:\n",
        "    # Load Whisper model if needed\n",
        "    whisper_model_obj = None\n",
        "    if auto_transcribe:\n",
        "        print(f\"üéØ Loading Whisper model ({whisper_model})...\")\n",
        "        whisper_model_obj = whisper.load_model(whisper_model)\n",
        "        print(\"‚úÖ Whisper model loaded\\n\")\n",
        "    \n",
        "    # Process files\n",
        "    entries = []\n",
        "    \n",
        "    for i, audio_file in enumerate(tqdm(audio_files, desc=\"Processing audio\"), 1):\n",
        "        # Validate audio\n",
        "        info = validate_audio(audio_file)\n",
        "        if not info['valid']:\n",
        "            print(f\"  ‚ö†Ô∏è Skipping {audio_file.name}: {info['error']}\")\n",
        "            continue\n",
        "        \n",
        "        # Get transcript\n",
        "        text = None\n",
        "        \n",
        "        # Check for existing transcript\n",
        "        if transcript_dir:\n",
        "            txt_file = Path(transcript_dir) / f\"{audio_file.stem}.txt\"\n",
        "            if txt_file.exists():\n",
        "                text = txt_file.read_text(encoding='utf-8').strip()\n",
        "        \n",
        "        # Auto-transcribe if needed\n",
        "        if text is None and auto_transcribe and whisper_model_obj:\n",
        "            text = transcribe_audio(audio_file, whisper_model_obj)\n",
        "        \n",
        "        # Use placeholder if still no text\n",
        "        if text is None:\n",
        "            text = \"Please transcribe this audio.\"\n",
        "        \n",
        "        # Format with speaker prefix\n",
        "        if speaker_prefix and not text.startswith(\"Speaker\"):\n",
        "            text = f\"{speaker_prefix}: {text}\"\n",
        "        \n",
        "        # Create entry\n",
        "        entry = {\n",
        "            'text': text,\n",
        "            'audio': str(audio_file.absolute()),\n",
        "            'metadata': {\n",
        "                'duration': info['duration'],\n",
        "                'sample_rate': info['sample_rate'],\n",
        "                'filename': audio_file.name\n",
        "            }\n",
        "        }\n",
        "        entries.append(entry)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processed {len(entries)} audio files\")\n",
        "    \n",
        "    # Split dataset\n",
        "    if val_split > 0 and len(entries) > 1:\n",
        "        random.shuffle(entries)\n",
        "        split_idx = int(len(entries) * (1 - val_split))\n",
        "        train_entries = entries[:split_idx]\n",
        "        val_entries = entries[split_idx:]\n",
        "        \n",
        "        # Write files\n",
        "        train_file = output_jsonl.with_suffix('.train.jsonl')\n",
        "        val_file = output_jsonl.with_suffix('.val.jsonl')\n",
        "        \n",
        "        with open(train_file, 'w', encoding='utf-8') as f:\n",
        "            for entry in train_entries:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "        \n",
        "        with open(val_file, 'w', encoding='utf-8') as f:\n",
        "            for entry in val_entries:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "        \n",
        "        # Also write combined\n",
        "        with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
        "            for entry in entries:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "        \n",
        "        print(f\"üìÑ Train: {train_file} ({len(train_entries)} samples)\")\n",
        "        print(f\"üìÑ Val: {val_file} ({len(val_entries)} samples)\")\n",
        "    else:\n",
        "        # Write single file\n",
        "        with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
        "            for entry in entries:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "        print(f\"üìÑ Dataset: {output_jsonl} ({len(entries)} samples)\")\n",
        "    \n",
        "    # Print statistics\n",
        "    total_duration = sum(e['metadata']['duration'] for e in entries)\n",
        "    avg_duration = total_duration / len(entries) if entries else 0\n",
        "    print(f\"\\nüìä Statistics:\")\n",
        "    print(f\"  Total samples: {len(entries)}\")\n",
        "    print(f\"  Total duration: {total_duration/60:.1f} minutes\")\n",
        "    print(f\"  Average duration: {avg_duration:.1f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 5: Train the Model\n",
        "\n",
        "Configure and start training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Training Configuration\n",
        "\n",
        "# @markdown ### Model Settings\n",
        "model_size = \"1.5B\"  # @param [\"1.5B\", \"7B\"]\n",
        "\n",
        "# @markdown ### Training Preset\n",
        "preset = \"default\"  # @param [\"fast\", \"default\", \"quality\"]\n",
        "\n",
        "# @markdown ### Training Duration\n",
        "num_epochs = 5  # @param {type: \"slider\", min: 1, max: 20, step: 1}\n",
        "\n",
        "# @markdown ### Advanced Settings\n",
        "learning_rate = 2.5e-5  # @param {type: \"number\"}\n",
        "batch_size = 4  # @param {type: \"slider\", min: 1, max: 8, step: 1}\n",
        "gradient_accumulation = 16  # @param {type: \"slider\", min: 1, max: 64, step: 1}\n",
        "lora_r = 8  # @param {type: \"slider\", min: 4, max: 64, step: 4}\n",
        "lora_alpha = 32  # @param {type: \"slider\", min: 8, max: 256, step: 8}\n",
        "\n",
        "# @markdown ### Memory Optimization\n",
        "gradient_checkpointing = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown ### Logging\n",
        "use_wandb = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# Preset configurations\n",
        "presets = {\n",
        "    'fast': {'epochs': 1, 'lr': 5e-5, 'desc': 'Quick testing'},\n",
        "    'default': {'epochs': 5, 'lr': 2.5e-5, 'desc': 'Balanced quality/speed'},\n",
        "    'quality': {'epochs': 10, 'lr': 1e-5, 'desc': 'Best quality'}\n",
        "}\n",
        "\n",
        "model_names = {\n",
        "    '1.5B': 'aoi-ot/VibeVoice-Base',\n",
        "    '7B': 'aoi-ot/VibeVoice-Large'\n",
        "}\n",
        "\n",
        "# Use preset values if not overridden\n",
        "if preset in presets:\n",
        "    preset_config = presets[preset]\n",
        "    final_epochs = preset_config['epochs'] if num_epochs == 5 else num_epochs\n",
        "    final_lr = preset_config['lr'] if learning_rate == 2.5e-5 else learning_rate\n",
        "else:\n",
        "    final_epochs = num_epochs\n",
        "    final_lr = learning_rate\n",
        "\n",
        "print(\"‚öôÔ∏è Training Configuration:\")\n",
        "print(f\"  Model: {model_size} ({model_names[model_size]})\")\n",
        "print(f\"  Preset: {preset}\")\n",
        "print(f\"  Epochs: {final_epochs}\")\n",
        "print(f\"  Learning rate: {final_lr}\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Gradient accumulation: {gradient_accumulation}\")\n",
        "print(f\"  LoRA r: {lora_r}, alpha: {lora_alpha}\")\n",
        "print(f\"  Gradient checkpointing: {gradient_checkpointing}\")\n",
        "print(f\"  W&B logging: {use_wandb}\")\n",
        "\n",
        "# Calculate effective batch size\n",
        "effective_batch = batch_size * gradient_accumulation\n",
        "print(f\"\\nüìä Effective batch size: {effective_batch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Start Training\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Determine model path\n",
        "model_repo = model_names[model_size]\n",
        "local_model_dir = MODELS_DIR / model_repo.replace(\"/\", \"--\")\n",
        "model_path = str(local_model_dir) if local_model_dir.exists() else model_repo\n",
        "\n",
        "# Determine dataset path\n",
        "train_dataset = DATA_DIR / \"dataset.train.jsonl\"\n",
        "val_dataset = DATA_DIR / \"dataset.val.jsonl\"\n",
        "\n",
        "if not train_dataset.exists():\n",
        "    train_dataset = DATA_DIR / \"dataset.jsonl\"\n",
        "\n",
        "# Build training command\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.finetune_vibevoice_lora\",\n",
        "    \"--model_name_or_path\", model_path,\n",
        "    \"--processor_name_or_path\", \"src/vibevoice/processor\",\n",
        "    \"--output_dir\", str(OUTPUT_DIR),\n",
        "    \"--text_column_name\", \"text\",\n",
        "    \"--audio_column_name\", \"audio\",\n",
        "    \"--train_jsonl\", str(train_dataset),\n",
        "    \"--remove_unused_columns\", \"False\",\n",
        "    \"--bf16\", \"True\",\n",
        "    \"--do_train\",\n",
        "    \"--per_device_train_batch_size\", str(batch_size),\n",
        "    \"--gradient_accumulation_steps\", str(gradient_accumulation),\n",
        "    \"--num_train_epochs\", str(final_epochs),\n",
        "    \"--learning_rate\", str(final_lr),\n",
        "    \"--lora_r\", str(lora_r),\n",
        "    \"--lora_alpha\", str(lora_alpha),\n",
        "    \"--lora_target_modules\", \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\",\n",
        "    \"--save_steps\", \"100\",\n",
        "    \"--logging_steps\", \"10\",\n",
        "    \"--lr_scheduler_type\", \"cosine\",\n",
        "    \"--warmup_ratio\", \"0.03\",\n",
        "    \"--max_grad_norm\", \"0.8\",\n",
        "    \"--gradient_clipping\",\n",
        "    \"--ddpm_batch_mul\", \"4\",\n",
        "    \"--diffusion_loss_weight\", \"1.4\",\n",
        "    \"--ce_loss_weight\", \"0.04\",\n",
        "    \"--train_diffusion_head\", \"True\",\n",
        "    \"--voice_prompt_drop_rate\", \"0.2\",\n",
        "]\n",
        "\n",
        "# Add validation if exists\n",
        "if val_dataset.exists():\n",
        "    cmd.extend([\"--validation_jsonl\", str(val_dataset)])\n",
        "    cmd.extend([\"--eval_steps\", \"100\"])\n",
        "\n",
        "# Add optional args\n",
        "if gradient_checkpointing:\n",
        "    cmd.append(\"--gradient_checkpointing\")\n",
        "\n",
        "if use_wandb:\n",
        "    cmd.extend([\"--report_to\", \"wandb\"])\n",
        "else:\n",
        "    cmd.extend([\"--report_to\", \"none\"])\n",
        "\n",
        "# Run training\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "print(\"Command:\")\n",
        "print(\"  \" + \" \\\\\n",
        "    \".join(cmd))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Run with output streaming\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True,\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "process.wait()\n",
        "\n",
        "if process.returncode == 0:\n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training failed with code {process.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 6: Save Results\n",
        "\n",
        "Save your trained model to Google Drive or download it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Save to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create save directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_name = f\"vibevoice_finetuned_{model_size}_{timestamp}\"\n",
        "save_dir = Path(f\"/content/drive/MyDrive/{save_name}\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üíæ Saving model to: {save_dir}\")\n",
        "\n",
        "# Copy files\n",
        "lora_dir = OUTPUT_DIR / \"lora\"\n",
        "if lora_dir.exists():\n",
        "    shutil.copytree(lora_dir, save_dir / \"lora\", dirs_exist_ok=True)\n",
        "    print(f\"  ‚úÖ LoRA adapter saved\")\n",
        "\n",
        "# Copy checkpoints\n",
        "checkpoint_dirs = sorted(OUTPUT_DIR.glob(\"checkpoint-*\"))\n",
        "if checkpoint_dirs:\n",
        "    latest_checkpoint = checkpoint_dirs[-1]\n",
        "    shutil.copytree(latest_checkpoint, save_dir / latest_checkpoint.name, dirs_exist_ok=True)\n",
        "    print(f\"  ‚úÖ Latest checkpoint saved: {latest_checkpoint.name}\")\n",
        "\n",
        "# Copy config\n",
        "config_file = REPO_DIR / \"src\" / \"vibevoice\" / \"processor\" / \"preprocessor_config.json\"\n",
        "if config_file.exists():\n",
        "    shutil.copy(config_file, save_dir)\n",
        "    print(f\"  ‚úÖ Processor config saved\")\n",
        "\n",
        "# Save training info\n",
        "info = {\n",
        "    \"model_size\": model_size,\n",
        "    \"preset\": preset,\n",
        "    \"num_epochs\": final_epochs,\n",
        "    \"learning_rate\": final_lr,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"gradient_accumulation\": gradient_accumulation,\n",
        "    \"lora_r\": lora_r,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "with open(save_dir / \"training_info.json\", \"w\") as f:\n",
        "    json.dump(info, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Model saved to Google Drive!\")\n",
        "print(f\"   Path: {save_dir}\")\n",
        "print(f\"\\nüìÇ Contents:\")\n",
        "for item in save_dir.iterdir():\n",
        "    print(f\"   {item.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Download Model (Alternative)\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Create zip file\n",
        "zip_path = BASE_DIR / \"model_export.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add LoRA adapter\n",
        "    lora_dir = OUTPUT_DIR / \"lora\"\n",
        "    if lora_dir.exists():\n",
        "        for file in lora_dir.rglob(\"*\"):\n",
        "            if file.is_file():\n",
        "                arcname = f\"lora/{file.relative_to(lora_dir)}\"\n",
        "                zipf.write(file, arcname)\n",
        "    \n",
        "    # Add checkpoints\n",
        "    for checkpoint_dir in OUTPUT_DIR.glob(\"checkpoint-*\"):\n",
        "        for file in checkpoint_dir.rglob(\"*\"):\n",
        "            if file.is_file():\n",
        "                arcname = f\"{checkpoint_dir.name}/{file.relative_to(checkpoint_dir)}\"\n",
        "                zipf.write(file, arcname)\n",
        "\n",
        "print(f\"üì¶ Created zip: {zip_path}\")\n",
        "print(f\"   Size: {zip_path.stat().st_size / (1024*1024):.1f} MB\")\n",
        "\n",
        "# Download\n",
        "files.download(str(zip_path))\n",
        "print(\"\\n‚¨áÔ∏è Download started!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Utilities\n",
        "\n",
        "Helper functions for inspection and debugging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Check Dataset\n",
        "\n",
        "import json\n",
        "\n",
        "dataset_file = DATA_DIR / \"dataset.jsonl\"\n",
        "\n",
        "if dataset_file.exists():\n",
        "    with open(dataset_file) as f:\n",
        "        entries = [json.loads(line) for line in f]\n",
        "    \n",
        "    print(f\"üìä Dataset Statistics:\")\n",
        "    print(f\"  Total entries: {len(entries)}\")\n",
        "    \n",
        "    if entries:\n",
        "        durations = [e.get('metadata', {}).get('duration', 0) for e in entries]\n",
        "        total_duration = sum(durations)\n",
        "        \n",
        "        print(f\"  Total duration: {total_duration/60:.1f} minutes\")\n",
        "        print(f\"  Average duration: {total_duration/len(entries):.1f} seconds\")\n",
        "        print(f\"\\nüìÑ Sample entries:\")\n",
        "        \n",
        "        for i, entry in enumerate(entries[:3], 1):\n",
        "            print(f\"\\n  Entry {i}:\")\n",
        "            print(f\"    Text: {entry.get('text', 'N/A')[:100]}...\")\n",
        "            print(f\"    Audio: {entry.get('audio', 'N/A')}\")\n",
        "            print(f\"    Duration: {entry.get('metadata', {}).get('duration', 'N/A')}s\")\n",
        "else:\n",
        "    print(\"‚ùå Dataset not found. Run dataset preparation first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Check Output Directory\n",
        "\n",
        "if OUTPUT_DIR.exists():\n",
        "    print(f\"üìÇ Output directory: {OUTPUT_DIR}\\n\")\n",
        "    \n",
        "    for item in sorted(OUTPUT_DIR.iterdir()):\n",
        "        if item.is_dir():\n",
        "            size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())\n",
        "            size_mb = size / (1024 * 1024)\n",
        "            print(f\"  üìÅ {item.name}/ ({size_mb:.1f} MB)\")\n",
        "        else:\n",
        "            size_kb = item.stat().st_size / 1024\n",
        "            print(f\"  üìÑ {item.name} ({size_kb:.1f} KB)\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No output directory yet. Training hasn't started or completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Clean Up (Free GPU Memory)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "print(\"üßπ Cleaning up...\")\n",
        "\n",
        "# Clear CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    print(\"  ‚úÖ CUDA cache cleared\")\n",
        "\n",
        "# Garbage collection\n",
        "gc.collect()\n",
        "print(\"  ‚úÖ Garbage collected\")\n",
        "\n",
        "# Show memory status\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"\\nüìä GPU Memory:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Cleanup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö References\n",
        "\n",
        "- Original VibeVoice: https://github.com/microsoft/VibeVoice\n",
        "- Fine-tuning repository: https://github.com/voicepowered-ai/VibeVoice-finetuning\n",
        "- This Colab notebook project: https://github.com/sruckh/VibeVoice-finetune-easy"
      ]
    }
  ]
}
